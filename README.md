# transformer-study
Attention Mechanism과 Transformer 구조를 쉽게 이해하기 위한 정리 및 구현 레포입니다. Bahdanau/Luong Attention, Multi-Head Attention, Positional Encoding 등을 다룹니다.
