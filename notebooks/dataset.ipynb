{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data/opensubtitles_en_ko\"\n",
    "\n",
    "# Load tokenizer models\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_ko = spacy.load(\"ko_core_news_sm\")\n",
    "\n",
    "# Tokenizer functions\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_ko(text):\n",
    "    return [tok.text for tok in spacy_ko.tokenizer(text)]\n",
    "\n",
    "# Load dataset\n",
    "src_file = os.path.join(data_dir, \"OpenSubtitles.en-ko.en\")\n",
    "trg_file = os.path.join(data_dir, \"OpenSubtitles.en-ko.ko\")\n",
    "\n",
    "src_texts, trg_texts = [], []\n",
    "with open(src_file, \"r\", encoding=\"utf-8\") as f_en, open(trg_file, \"r\", encoding=\"utf-8\") as f_ko:\n",
    "    for en_line, ko_line in zip(f_en, f_ko):\n",
    "        src_texts.append(en_line.strip())\n",
    "        trg_texts.append(ko_line.strip())\n",
    "\n",
    "# Save dataset\n",
    "save_data = lambda data, path: json.dump(data, open(path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=4)\n",
    "save_data(src_texts, os.path.join(data_dir, \"src_texts.json\"))\n",
    "save_data(trg_texts, os.path.join(data_dir, \"trg_texts.json\"))\n",
    "\n",
    "# Load vocabulary\n",
    "src_vocab = {word: i for i, word in enumerate([\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"] + list(set([tok for text in src_texts for tok in tokenize_en(text)])))}\n",
    "trg_vocab = {word: i for i, word in enumerate([\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"] + list(set([tok for text in trg_texts for tok in tokenize_ko(text)])))}\n",
    "\n",
    "# Save vocab\n",
    "save_data(src_vocab, os.path.join(data_dir, \"src_vocab.json\"))\n",
    "save_data(trg_vocab, os.path.join(data_dir, \"trg_vocab.json\"))\n",
    "\n",
    "# Load dataset\n",
    "src_texts = json.load(open(os.path.join(data_dir, \"src_texts.json\"), \"r\", encoding=\"utf-8\"))\n",
    "trg_texts = json.load(open(os.path.join(data_dir, \"trg_texts.json\"), \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_vocab, trg_vocab, max_len=50):\n",
    "        self.src_texts = src_texts\n",
    "        self.trg_texts = trg_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_seq = self.text_to_tensor(self.src_texts[idx], self.src_vocab)\n",
    "        trg_seq = self.text_to_tensor(self.trg_texts[idx], self.trg_vocab)\n",
    "        return src_seq, trg_seq\n",
    "    \n",
    "    def text_to_tensor(self, text, vocab):\n",
    "        tokens = tokenize_en(text) if vocab == src_vocab else tokenize_ko(text)\n",
    "        token_ids = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        token_ids = token_ids[:self.max_len] + [vocab[\"<PAD>\"]] * (self.max_len - len(token_ids))\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TranslationDataset(src_texts, trg_texts, src_vocab, trg_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 문장 예시:\n",
      "Through the snow and sleet and hail, through the blizzard, through the gales, through the wind and through the rain, over mountain, over plain, through the blinding lightning flash, and the mighty thunder crash,\n",
      "\n",
      "한국어 문장 예시:\n",
      "폭설이 내리고 우박, 진눈깨비가 퍼부어도 눈보라가 몰아쳐도 강풍이 불고 비바람이 휘몰아쳐도\n"
     ]
    }
   ],
   "source": [
    "# JSON 파일에서 로드한 원본 텍스트 확인\n",
    "print(\"영어 문장 예시:\")\n",
    "print(src_texts[0])  # 첫 번째 영어 문장\n",
    "\n",
    "print(\"\\n한국어 문장 예시:\")\n",
    "print(trg_texts[0])  # 첫 번째 한국어 문장\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized English:\n",
      "['through', 'the', 'snow', 'and', 'sleet', 'and', 'hail', ',', 'through', 'the', 'blizzard', ',', 'through', 'the', 'gales', ',', 'through', 'the', 'wind', 'and', 'through', 'the', 'rain', ',', 'over', 'mountain', ',', 'over', 'plain', ',', 'through', 'the', 'blinding', 'lightning', 'flash', ',', 'and', 'the', 'mighty', 'thunder', 'crash', ',']\n",
      "Tokenized Korean:\n",
      "['폭설이', '내리고', '우박', ',', '진눈깨비가', '퍼부어도', '눈보라가', '몰아쳐도', '강풍이', '불고', '비바람이', '휘몰아쳐도']\n"
     ]
    }
   ],
   "source": [
    "# 영어 문장의 토큰화 결과 확인\n",
    "example_en = src_texts[0]\n",
    "print(\"Tokenized English:\")\n",
    "print(tokenize_en(example_en))\n",
    "\n",
    "# 한국어 문장의 토큰화 결과 확인\n",
    "example_ko = trg_texts[0]\n",
    "print(\"Tokenized Korean:\")\n",
    "print(tokenize_ko(example_ko))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'hello' 단어의 영어 vocab 인덱스: 80975\n"
     ]
    }
   ],
   "source": [
    "word = \"hello\"\n",
    "print(f\"'{word}' 단어의 영어 vocab 인덱스: {src_vocab.get(word, src_vocab['<UNK>'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source tensor (영어):\n",
      "tensor([30083, 76073, 27677,  9609, 16718,  9609,  1628, 46741, 30083, 76073,\n",
      "        80418, 46741, 30083, 76073, 79897, 46741, 30083, 76073, 74471,  9609,\n",
      "        30083, 76073, 74891, 46741,  6220, 35610, 46741,  6220, 74953, 46741,\n",
      "        30083, 76073, 14288, 24043, 65832, 46741,  9609, 76073, 37537, 71103,\n",
      "         2302, 46741,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "\n",
      "Target tensor (한국어):\n",
      "tensor([175501, 443087, 646316, 126343, 522098, 426435, 262328, 249459, 576767,\n",
      "        267776, 254033, 254510,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
      "             0,      0,      0,      0,      0])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 첫 번째 샘플 확인\n",
    "src_tensor, trg_tensor = dataset[0]\n",
    "\n",
    "print(\"Source tensor (영어):\")\n",
    "print(src_tensor)  # 텐서 형태로 변환된 영어 문장 (길이 max_len)\n",
    "\n",
    "print(\"\\nTarget tensor (한국어):\")\n",
    "print(trg_tensor)  # 텐서 형태로 변환된 한국어 문장 (길이 max_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 - Source batch shape: torch.Size([32, 50])\n",
      "Batch 0 - Target batch shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# DataLoader에서 한 배치를 확인\n",
    "for batch_idx, (src_batch, trg_batch) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx} - Source batch shape: {src_batch.shape}\")\n",
    "    print(f\"Batch {batch_idx} - Target batch shape: {trg_batch.shape}\")\n",
    "    # 예시로 첫 배치만 확인하고 루프 탈출\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maadf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[38;5;241m.\u001b[39mlower\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'text'"
     ]
    }
   ],
   "source": [
    "\"aadf\".text.lower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '!', 'this', 'is', 'nlp', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
