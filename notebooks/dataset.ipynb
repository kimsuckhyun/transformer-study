{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import spacy\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Set data directory\n",
    "data_dir = \"../data/opensubtitles_en_ko\"\n",
    "\n",
    "# Load tokenizer models\n",
    "spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "spacy_ko = spacy.load(\"ko_core_news_sm\")\n",
    "\n",
    "# Tokenizer functions\n",
    "def tokenize_en(text):\n",
    "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "def tokenize_ko(text):\n",
    "    return [tok.text for tok in spacy_ko.tokenizer(text)]\n",
    "\n",
    "# Load dataset\n",
    "src_file = os.path.join(data_dir, \"OpenSubtitles.en-ko.en\")\n",
    "trg_file = os.path.join(data_dir, \"OpenSubtitles.en-ko.ko\")\n",
    "\n",
    "src_texts, trg_texts = [], []\n",
    "with open(src_file, \"r\", encoding=\"utf-8\") as f_en, open(trg_file, \"r\", encoding=\"utf-8\") as f_ko:\n",
    "    for en_line, ko_line in zip(f_en, f_ko):\n",
    "        src_texts.append(en_line.strip())\n",
    "        trg_texts.append(ko_line.strip())\n",
    "\n",
    "# Save dataset\n",
    "save_data = lambda data, path: json.dump(data, open(path, \"w\", encoding=\"utf-8\"), ensure_ascii=False, indent=4)\n",
    "save_data(src_texts, os.path.join(data_dir, \"src_texts.json\"))\n",
    "save_data(trg_texts, os.path.join(data_dir, \"trg_texts.json\"))\n",
    "\n",
    "# Load vocabulary\n",
    "src_vocab = {word: i for i, word in enumerate([\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"] + list(set([tok for text in src_texts for tok in tokenize_en(text)])))}\n",
    "trg_vocab = {word: i for i, word in enumerate([\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"] + list(set([tok for text in trg_texts for tok in tokenize_ko(text)])))}\n",
    "\n",
    "# Save vocab\n",
    "save_data(src_vocab, os.path.join(data_dir, \"src_vocab.json\"))\n",
    "save_data(trg_vocab, os.path.join(data_dir, \"trg_vocab.json\"))\n",
    "\n",
    "# Load dataset\n",
    "src_texts = json.load(open(os.path.join(data_dir, \"src_texts.json\"), \"r\", encoding=\"utf-8\"))\n",
    "trg_texts = json.load(open(os.path.join(data_dir, \"trg_texts.json\"), \"r\", encoding=\"utf-8\"))\n",
    "\n",
    "# Custom Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_texts, trg_texts, src_vocab, trg_vocab, max_len=50):\n",
    "        self.src_texts = src_texts\n",
    "        self.trg_texts = trg_texts\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src_seq = self.text_to_tensor(self.src_texts[idx], self.src_vocab)\n",
    "        trg_seq = self.text_to_tensor(self.trg_texts[idx], self.trg_vocab)\n",
    "        return src_seq, trg_seq\n",
    "    \n",
    "    def text_to_tensor(self, text, vocab):\n",
    "        tokens = tokenize_en(text) if vocab == src_vocab else tokenize_ko(text)\n",
    "        token_ids = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        token_ids = token_ids[:self.max_len] + [vocab[\"<PAD>\"]] * (self.max_len - len(token_ids))\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = TranslationDataset(src_texts, trg_texts, src_vocab, trg_vocab)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
